# Regression Analysis å›å½’åˆ†æ

ç®€å•ä»‹ç»åšâ€œå›å½’åˆ†æâ€çš„æ­¥éª¤PACEï¼šä»æ‹†è§£é—®é¢˜åˆ°ç»“è®ºé‡Šä¹‰<br>
å­¦ä¹ ä¸¤ç§å›å½’åˆ†ææ–¹æ³•linear and logisticï¼šçº¿æ€§å›å½’å’Œé€»è¾‘å›å½’<br>
äº†è§£è¿™ä¸¤ç§å›å½’åˆ†æå¦‚ä½•è§£å†³å®é™…é—®é¢˜ã€‚

## M1 Introduction
### å›å½’åˆ†æå››æ­¥æ³• PACE

P è®¡åˆ’ planâ€”â€”å……åˆ†äº†è§£é¡¹ç›®éœ€æ±‚å’Œå¯ç”¨èµ„æº<br>
A åˆ†æ Analyzeâ€”â€”è¯•è¯•å“ªä¸ªæ¨¡å‹æ›´é€‚åˆç”¨æ¥åšå›å½’åˆ†æ<br>
C å»ºæ¨¡ Constructâ€”â€”å»ºæ¨¡å¹¶å¤„ç†æ•°æ®<br>
E æ‰§è¡Œ Executeâ€”â€”æ ¹æ®è¿ç®—ç»“æœè¿›è¡Œåˆ†æï¼Œå¾—åˆ°ç»“è®ºï¼Œå¹¶è§£é‡Š

### Liner Regression çº¿æ€§å›å½’

é€‚åˆç”¨æ¥æè¿°ï¼š<br>
ä¸€ç§å› ç´ Yä¼šè·Ÿéšç€å¦ä¸€ç§å› ç´ Xçš„å˜åŒ–è€Œæ”¹å˜ã€‚ï¼ˆå½±å“å› ç´ Xå¯ä»¥ä¸æ­¢ä¸€ç§ï¼‰<br>
ä»–ä»¬ä¹‹é—´çš„ä¼´éšå…³ç³»æ˜¯çº¿æ€§çš„ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼ŒYä¼šæŒ‰æŸç§æ¯”ä¾‹ï¼ŒéšXçš„å¢åŠ è€Œå¢åŠ ï¼ˆæˆ–å‡å°‘ï¼‰

Î¼{Y|X} = Î²<sub>0</sub>+Î²<sub>1</sub>X

- Yï¼ŒDependent variable è¢«å½±å“å› ç´ 
- Xï¼ŒIndependent variable(s) å½±å“å› ç´ ï¼Œè‡ªå˜é‡
- Regression coefficients å›å½’æ–¹ç¨‹ç³»æ•°
	- Î²<sub>1</sub>ï¼ŒSlope æ–œç‡
	- Î²<sub>0</sub>ï¼ŒIntercept æˆªè·
- OLSï¼Œordinary least squares estimationï¼Œæ¨ç®—çº¿æ€§å›å½’å…¬å¼çš„æ–¹æ³•
	- loss functiion ç†æƒ³Yä¸å®é™…è§‚æµ‹åˆ°çš„Yä¹‹é—´çš„å·®

### Logistic Regression é€»è¾‘å›å½’

é€‚åˆç”¨æ¥æè¿°ï¼š<br>
éšç€æŸç§å› ç´ Xçš„å˜åŒ–ï¼ŒYçš„å½’ç±»ä¼šæ”¹å˜ã€‚ï¼ˆå½±å“å› ç´ Xå¯ä»¥ä¸æ­¢ä¸€ç§ï¼‰<br>
Yçš„å½’ç±»æ˜¯å¯ä»¥è®¡æ•°çš„ï¼Œä»ä¸¤ç±»åˆ°æœ‰é™å¤šç±»ã€‚

Î¼{Y|X} = Prob(Y = n|X) = p

- pï¼Œprobability å½“Xæ˜¯æŸä¸ªå€¼çš„æ—¶å€™ï¼ŒYå±äºç¬¬nç§å½’ç±»çš„å¯èƒ½æ€§æ˜¯å¤šå°‘ã€‚
- link function æ¨ç®—é€»è¾‘å›å½’å…¬ç¤ºçš„æ–¹æ³•

### Correlation is not Causation 

ä¸¤ç§å› ç´ ç›¸å…³ï¼Œå¹¶ä¸ç­‰äºè°èƒ½å¼•å‘è°ã€‚è¿™ä¸æ˜¯ä¸€ç§å› æœå…³ç³»ã€‚

## M2 Simple linear regression ä¸€å…ƒçº¿æ€§å›å½’æ¨¡å‹

Yçš„å˜åŒ–åªå—ä¸€ä¸ªå› ç´ Xå½±å“ã€‚<br>
åœ¨ä¸€ä¸ªäºŒç»´åæ ‡ç³»é‡Œï¼Œæ¯å¯¹ç›¸å…³å› ç´ Xå’ŒYçš„å…³ç³»èƒ½æŠ˜å°„æˆä¸€ä¸ªç‚¹ã€‚æ‰¾å‡ºä¸€æ¡ç›´çº¿ï¼ˆçš„æ–¹ç¨‹ï¼‰ï¼Œæœ€èƒ½è¡¨ç¤ºè¿™äº›ç‚¹çš„åˆ†å¸ƒè¶‹åŠ¿ã€‚

### OLS æœ€å°è¯¯å·®æ³•æ¨ç®—å›å½’æ–¹ç¨‹

minimizing loss function / error

- Îµï¼ŒResidual æ®‹å·®ï¼ŒYè§‚æµ‹å€¼ ä¸ Yçš„æ¨æµ‹å€¼ ä¹‹é—´çš„å·®å¼‚

	Î£(Îµ) == 0 ä½¿ç”¨OLSæ³•ï¼Œæ®‹å·®å’Œæ°¸è¿œä¸ºé›¶

- SSRï¼Œsum of squared residuals æ®‹å·®å¹³æ–¹å’Œ
- åœ¨æ‰€æœ‰çš„ç›´çº¿é€‰é¡¹é‡Œï¼Œæ‰¾é‚£æ¡è®©SSRå€¼æœ€ä½çš„ï¼ˆçº¿ï¼‰
	
### ç”¨rè®¡ç®—å›å½’çº¿æ–¹ç¨‹
- rï¼ŒPearson's correlation / linear correlation coefficient çº¿æ€§ç›¸å…³ç³»æ•°	

	- råœ¨[-1,1]ä¹‹é—´

		rçš„ç»å¯¹å€¼è¶Šå¤§ï¼Œæ ·æœ¬ç‚¹è¶Šåƒä¸€æ¡ç›´çº¿ï¼›rè¶Šå°ï¼Œæ ·æœ¬ç‚¹è¶Šæ¥è¿‘ä¸€å›¢æ— åºäº‘çŠ¶æ•£ç‚¹

	- r æ­£è´Ÿçš„å«ä¹‰

		r>0æ„å‘³ç€æ–œç‡æ˜¯æ­£æ•°ï¼Œå€¾æ–œè§’åº¦â†—<br>
		r<0æ„å‘³ç€æ–œç‡æ˜¯è´Ÿæ•°ï¼Œå€¾æ–œè§’åº¦â†˜

	- rçš„æ•°å€¼ä¸æ˜¯æ–œç‡
	- r = covariance(X,Y)åæ–¹å·®/(SD X)(SD Y)æ ‡å‡†å·®
	
- æœ‰ä¸¤æ¡å®šå¾‹
	- Xçš„å¹³å‡å€¼å’ŒYçš„å¹³å‡å€¼æ°¸è¿œä¼šè½åœ¨å›å½’çº¿ä¸Š
	- å¦‚æœXå¢åŠ 1ä¸ªXæ ‡å‡†å·®ï¼Œé‚£ä¹ˆYä¼šå¢åŠ rä¸ªYæ ‡å‡†å·®
	
- å›å½’çº¿çš„æ–œç‡Î²<sub>1</sub> = r(SD Y)/SD X
		
### ç”¨Pythonæ¥å®Œæˆæ‰€æœ‰è®¡ç®—
- A é¢„åˆ†æé˜¶æ®µ
	- è§‚å¯Ÿä¸¤ä¸¤æ•£ç‚¹å›¾çŸ©é˜µ
			
		```python
		# æˆ‘æœäº†ä¸€ä¸‹ï¼Œå¤šä¸ªå¸–å­éƒ½è¯´
		# ç”»ä¸¤ä¸¤æ•£ç‚¹å›¾å°±ç”¨seabornåº“
		import seaborn as sns		
		# ç»™æ¯ä¸¤ä¸ªå˜é‡ä¹‹é—´ç”»ä¸€å¹…æ•£ç‚¹å›¾
		sns.pairplot(origData)
		```

- C å»ºæ¨¡
	- Step1 Build a model
			
		```python
		# Subset Data æ¸…æ´—å¹¶é€‰æ‹©è¦è¿›è¡Œå›å½’åˆ†æçš„ä¸¤åˆ—æ•°æ®
		ols_data = origData[["column1/X", "column2/Y"]]
		# Write out formula å®šä¹‰Yå’ŒXåˆ†åˆ«æ˜¯å“ªåˆ—æ•°æ®
		ols_formula = "column2/Y ~ column1/X"
		# Import ols function
		from statsmodels.formula.api import ols
		# Build OLS, fit model to data ç”¨OLSæ–¹æ³•å»ºæ¨¡è®¡ç®—å‡ºå›å½’çº¿
		OLS = ols(formula = ols_formula, data = ols_data)
		model = OLS.fit()
		```
							
	- Step 2 Model evaluation

		P-valueï¼ŒConfidence Intervals

		```python
		# print statistics è¾“å‡ºæ¨¡å‹çš„å„é¡¹ç»Ÿè®¡æŒ‡æ ‡
		model.summary()

		# confidence band 
		sns.regplot(x="column1/X", y="column2/Y", data = ols_data)
		
		# Xï¼Œå–Xå€¼
		X = ols_data["column1/X"]
		# Yï¼Œç”¨é¢„æµ‹å…¬å¼Predictè·å¾—Yå€¼fitted_values
		fitted_values = model.predict(X)
		# Residualsï¼Œç”¨residå…¬å¼è·å¾—æ®‹å·®å€¼
		residuals = model.resid
		```
		
		Homoscedasticity

		```python
		# Residualsåœ¨0é™„è¿‘çš„åç§»é‡æ•£ç‚¹å›¾
		import matplotlib.pyplot as plt
		fig = sns.scatterplot(x=fitted_values, y=residuals)
		fig.axhline(0)
		fig.set_xlabel("Fitted Values")
		fig.set_ylabel("Residuals")
		plt.show()
		```
		
		Normality

		```python
		# Residualsçš„æŸ±çŠ¶å›¾
		fig = sns.histplot(residuals)
		fig.set_xlabel("Residual Value")
		fig.set_title("Histogram of Residuals")
		plt.show()
		# Q-Q plotå›¾	
		import statsmodels.api as sm
		fig = sm.qqplot(model.resid, line = 's')
		plt.show()
		```
			
		R<up>2</up>ï¼ŒMSE/MAE
		```python
		# å¯¼å…¥åº“sklearn
		from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
		# å‡è®¾å·²ç»æœ‰ä¸€ç»„Hold_out sample[["x_test","y_test"]]
		y_test_pred = model.predict(x_test)
		MSE = mean_squared_error(y_test,y_test_pred)
		MAE = mean_absolute_error(y_test,y_test_pred)
		R2 = r2_score(y_test,y_test_pred)
		print('MSE:',MSE,'/nMAE:',MAE,'/nR2:',R2)
		```		
		
- E å¯ä»¥ç”¨pythonæ¥å®ŒæˆData visualisationå·¥ä½œ
		
### å»ºæ¨¡åˆ†æè¿‡ç¨‹PACE

- A æ£€æŸ¥çº¿æ€§å›å½’å‡è®¾æ˜¯ä¸æ˜¯éƒ½æ»¡è¶³
	- Linearity ä¸¤ä¸ªå˜é‡XYä¹‹é—´æ˜¯å¦æ˜¯çº¿æ€§ç›¸å…³

		å¯ä»¥å…ˆæŠ“æ ·æœ¬ç”»ä¸ªæ•£ç‚¹å›¾ï¼Œçœ‹çœ‹è¿™äº›ç‚¹çš„åˆ†å¸ƒåƒä¸åƒç›´çº¿
	
	- Normality æ®‹å·®residualæ˜¯å¦æ˜¯normal distributedæ­£æ€åˆ†å¸ƒ

		å¿…é¡»è¦å»ºå®Œæ¨¡å‹åæ‰èƒ½æ£€éªŒï¼Œå¯ä»¥ç›´æ¥ç”»æŸ±çŠ¶å›¾ä¹Ÿå¯ä»¥ç”»Q-Q plotå›¾æ¥è§‚å¯Ÿåˆ¤æ–­

	- Independent observations é‡‡é›†çš„æ ·æœ¬ä¹‹é—´æ²¡æœ‰ç›¸äº’å½±å“
		ä¸»è¦é åˆ†ææ•°æ®é‡‡é›†æ­¥éª¤æ¥åˆ¤æ–­ï¼Œæˆ–è€…å»ºæ¨¡åè¾“å‡ºç»“æœæ˜¾ç¤ºæœ‰é—®é¢˜æ—¶èƒ½å‘ç°æ ·æœ¬ä¸ç¬¦åˆå‡è®¾
	
	- Homoscedasticity æ®‹å·®çš„åç§»é‡æŒç»­ä¸”å­˜åœ¨ä¸”æ•°å€¼éšæœºä½†æ¥è¿‘

		å¿…é¡»è¦å»ºå®Œæ¨¡å‹åæ‰èƒ½æ£€éªŒï¼ŒåŒæ ·å¯ä»¥é€šè¿‡ç”»å‡ºæ•£ç‚¹å›¾æ¥è§‚å¯Ÿåˆ¤æ–­
		
- C ç”¨é€‚åˆçš„å˜é‡å»ºæ¨¡ï¼Œå¾—åˆ°å„é¡¹ç»Ÿè®¡æ•°æ®
	- å»ºæ¨¡ï¼ˆå‚è§ï¼šç”¨Pythonæ¥å®Œæˆæ‰€æœ‰è®¡ç®—/C å»ºæ¨¡ï¼‰
	- è¯„ä¼°
		
		äº†è§£summaryç»™å‡ºçš„å„ä¸ªæŒ‡æ ‡éƒ½æ˜¯ä»€ä¹ˆæ„æ€
		- confidence band
		
			åœ¨å›å½’çº¿é™„è¿‘ï¼Œè½åœ¨ç½®ä¿¡åŒºé—´é‡Œæ‰€æœ‰çš„ç›´çº¿ç»„æˆçš„ä¸€ç‰‡åŒºåŸŸ<br>
			ç”¨æ ·æœ¬è¿›è¡Œå›å½’åˆ†æï¼Œæ€»æ˜¯å¯èƒ½ä¸æ•´ä½“çš„å›å½’åˆ†æç»“æœå­˜åœ¨åå·®
		
		- P-value			
		
			null hypothesisï¼ˆXYä¸å­˜åœ¨çº¿æ€§ç›¸å…³æ€§ï¼‰æˆç«‹çš„å¯èƒ½æ€§ï¼ˆæ¦‚ç‡ï¼‰<br>
			å¦‚æœPå°äºç½®ä¿¡åº¦ï¼ˆ5%ï¼‰ï¼Œnull hypothesisè¢«æ¨ç¿»<br>
			å¯ä»¥è®¤ä¸ºXYä¹‹é—´çº¿æ€§ç›¸å…³ï¼Œcoefficientä¸æ˜¯0
			
		- Confidence Intervals [0.025, 0.975]
		
			è¡¨ç¤ºæœ‰5%çš„æœºä¼šï¼Œæ–œç‡å’Œæˆªè·çš„ç½®ä¿¡èŒƒå›´å€¼ä¸èƒ½åŒ…å«å›å½’çº¿çš„çœŸå®å‚æ•°<br>
			åœ¨è¿™ä¸€åˆ—ä¸‹é¢ç»™å‡ºæ¥çš„æ•°æ®æ˜¯æˆªè·å’Œæ–œç‡çš„èŒƒå›´ï¼Œç”¨æ¥ç”»å‡ºcofidence bandåŒºåŸŸ
		
		å¸¸è§çš„è¯„ä¼°çŸ©é˜µ
		- R<sup>2</sup> å†³å®šç³»æ•° 0~1ä¹‹é—´
			
			ç”¨æ¥æè¿°Xå¯¹Yçš„å½±å“ç¨‹åº¦ã€‚è¶Šæ¥è¿‘äº1è¯´æ˜è¶Šé€‚åˆç”¨çº¿æ€§å›å½’åˆ†æã€‚
			çº¿æ€§ç›¸å…³ç³»æ•°rçš„å¹³æ–¹(æœ‰å¾…éªŒè¯ï¼Œè®¡ç®—å…¬å¼ä¸ä¸€æ ·)
			
		- MSEï¼Œmean squared error
			
			å¯¹outlieræ•æ„Ÿï¼Œå€¼è¶Šå°è¶Šå¥½
			
		- MAEï¼Œmean absolute error 
			
			åœ¨æœ‰outlieræ—¶ä½¿ç”¨ï¼Œå€¼è¶Šå°è¶Šå¥½ 
			
		- Hold-out sample 
			
			ä¸èƒ½æ˜¯ä¹‹å‰å»ºæ¨¡fit modelæ—¶ä½¿ç”¨è¿‡çš„æ•°æ®ï¼Œå¯ä»¥ç”¨è¿™ç»„æ–°æ•°æ®æ£€æµ‹ä»¥ä¸Šä¸‰ä¸ªå€¼
			
- E å¯¹å»ºæ¨¡ç»“æœè¿›è¡Œå…¨é¢è¯„ä¼°å’Œè§£é‡Š
	- è§£é‡Šæ¨¡å‹çš„ç»Ÿè®¡æŒ‡æ ‡éƒ½æ„å‘³ç€ä»€ä¹ˆï¼Œæ¯”å¦‚æ–œç‡è¡¨æ˜çš„Yå°†å¦‚ä½•å› ä¸ºXè€Œå˜åŒ–
	- å°½é‡å°†æ•°å­—è½¬åŒ–æˆæ˜“äºç†è§£çš„å›¾åƒã€åŠ¨ç”»ç­‰æ¥è¿›è¡Œè®²è§£æˆ–æ¼”ç¤º
	- æœ‰å¿…è¦æé†’å¬ä¼—æ¨¡å‹ä»å¯èƒ½åœ¨ä»€ä¹ˆæƒ…å†µä¸‹å¤±æ•ˆï¼Œæˆ–éœ€è¦è¿›è¡Œä¿®æ­£
	- ä½†è¦å°½é‡é¿å…ä½¿ç”¨æ™¦æ¶©çš„æœ¯è¯­ï¼Œå¦‚coefficients or P-value 
	- è¦æ³¨æ„åŒºåˆ†correlationå’Œcausationçš„åŒºåˆ«ï¼Œæˆ‘ä»¬å‡ ä¹æ— æ³•åœ¨è¿™é‡Œè¯æ˜causaiton

## M3 Multiple linear regression å¤šå…ƒçº¿æ€§å›å½’æ¨¡å‹

å½“ä¸æ­¢ä¸€ä¸ªå› ç´ å…±åŒå½±å“Yæ—¶ï¼Œå¼•å…¥å¤šå…ƒçº¿æ€§å›å½’æ¨¡å‹

Y = Î²<sub>0</sub>+Î²<sub>1</sub>X<sub>1</sub>+...+Î²<sub>n</sub>X<sub>n</sub>

### One hot encoding ç‹¬çƒ­ç¼–ç 

å¦‚æœå½±å“å› ç´ X<sub>i</sub>æ˜¯åˆ†ç±»å˜é‡ã€‚ç±»ä¼¼å¤šé€‰é¢˜çš„ç­”æ¡ˆï¼Œå¦‚ï¼šåšæˆ–è€…ä¸åšï¼Œé€‰æ‹©äº†ACæ²¡æœ‰é€‰BDEã€‚<br>
è¿™æ—¶ä¸ºäº†å¯ä»¥è¿›è¡Œå›å½’åˆ†æï¼Œè¦æŠŠX<sub>i</sub>æ‹†è§£æˆä¸€ç»„äºŒè¿›åˆ¶æ•°æ¥è¡¨ç¤ºå®ƒçš„å…¨éƒ¨ç‰¹å¾<br>
X<sub>i</sub>â†’X<sub>iA</sub>,X<sub>iB</sub>,...,X<sub>iN</sub><br>
äºŒè¿›åˆ¶çš„ä½æ•°å’ŒXæœ‰å‡ ä¸ªç‰¹å¾æœ‰å…³

### Interaction äº¤äº’é¡¹/äº¤å‰å˜é‡

è¿™ä¸ªæ¦‚å¿µæˆ‘è§‰å¾—æœ¬è¯¾è¯´çš„ä¸å¾ˆæ¸…æ¥šï¼Œçœ‹å®Œåæˆ‘è¿˜æœ‰å¥½å‡ ä¸ªç–‘é—®<br>
ä»€ä¹ˆæ—¶å€™åŠ å…¥äº¤äº’é¡¹ï¼Ÿä¸è®ºå½±å“å› ç´ æ˜¯åˆ†ç±»å˜é‡è¿˜æ˜¯è¿ç»­æ•°å€¼å˜é‡éƒ½å¯ä»¥å¼•å…¥äº¤äº’é¡¹ä¹ˆï¼Ÿå¦‚ä½•è§£è¯»äº¤äº’é¡¹ï¼Ÿ
æœ€åæ‰¾äº†ä¸€ç¯‡çŸ¥ä¹æ–‡ç« ç®—æ˜¯åŸºæœ¬çœ‹æ˜ç™½äº†ï¼Œé…åˆè¯„è®ºåŒºå°±æ›´å…¨é¢çš„å›ç­”äº†æˆ‘çš„ç–‘é—®ã€‚

[ä¸€æ–‡è½»æ¾çœ‹æ‡‚äº¤äº’ä½œç”¨](https://zhuanlan.zhihu.com/p/224990519 "")

### å¤šå…ƒçº¿æ€§å›å½’å‡è®¾
- Linearity 
	
	çœ‹æ¯ä¸€ä¸ªX<sub>i</sub>å’ŒYçš„æ•£ç‚¹å›¾æ˜¯ä¸æ˜¯åƒä¸€æ¡çº¿
	
- Independance, Normality, Homoscedasticity
	
	å®šä¹‰å’Œæ£€æµ‹éƒ½å’Œä¸€å…ƒçº¿æ€§å›å½’å‡è®¾ä¸€æ ·
	
- No multicollinerity assumption è‡ªå˜é‡ä¹‹é—´æ²¡æœ‰çº¿æ€§å…³ç³»å‡è®¾

	é€šè¿‡æ‰€æœ‰å˜é‡ä¹‹é—´çš„ä¸¤ä¸¤æ•£ç‚¹å›¾æ¥åˆ¤æ–­

	```python
	sns.pairplot()
	```
	å¦‚æœæ•£ç‚¹å›¾ä¸å¥½åˆ¤æ–­ï¼Œå¯ä»¥è®¡ç®—ä¸¤ä¸ªå˜é‡ä¹‹é—´çš„VIFå€¼ï¼ˆ1~âˆï¼‰ã€‚VIFè¶Šå¤§çº¿æ€§å…³ç³»è¶Šå¼ºã€‚
	
	```python
	from statsmodels.stats.outliers_influence import variance_inflation_factor
	X = df[['col_1', 'col_2', 'col_3']]
	vif = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]
	vif = zip(X, vif)
	print(list(vif))
	```
	é¿å…åŒæ—¶æŒ‘é€‰ä¸Šä¸¤ä¸ªæ˜æ˜¾æœ‰çº¿æ€§å…³ç³»çš„å˜é‡ä½œä¸ºX<sub>i</sub>&X<sub>j</sub>ï¼Œæˆ–æ˜¯å°†ä¸¤ä¸ªæœ‰å¾ˆå¼ºçº¿æ€§å…³ç³»çš„å˜é‡è½¬åŒ–æˆä¸€ä¸ªæ–°çš„å˜é‡ã€‚
		
### ç”¨pythonå»ºç«‹å¤šå…ƒå›å½’æ¨¡å‹
- C å»ºæ¨¡

	```python
	# å‡†å¤‡æ•°æ®
	X = origData[["col_1/X1","col_2/X2",...,"col_n/Xn"]]
	Y = origData[["col_0/Y"]]
	# å¯¼å…¥åº“
	from sklearn.model_selection import train_test_split
	# æŠŠæ•°æ®åˆ†æˆå»ºæ¨¡å’Œæµ‹è¯•ä¸¤éƒ¨åˆ†
	X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.3, random_state = 42]
	# å‡†å¤‡å»ºæ¨¡ç”¨api
	ols_data = pd.concat([X_train, y_train], axis = 1)
	# Write out formula å®šä¹‰Yå’ŒXåˆ†åˆ«æ˜¯å“ªåˆ—æ•°æ®
	ols_formula = "col_0/Y ~ col_1/X1 + C(col_2/categorical X2)+...+col_n/Xn"
	# Import ols function
	from statsmodels.formula.api import ols
	# Build OLS, fit model to data ç”¨OLSæ–¹æ³•å»ºæ¨¡è®¡ç®—å‡ºå›å½’çº¿
	OLS = ols(formula = ols_formula, data = ols_data)
	model = OLS.fit()	
	```

- E å„é¡¹ç»Ÿè®¡æŒ‡æ ‡çš„å«ä¹‰

### variable selection

é€‰æ‹©åŒ…å«ä»€ä¹ˆç‰¹å¾/å½±å“å› ç´ /è‡ªå˜é‡åˆ°å›å½’æ¨¡å‹é‡Œã€‚æ ¹æ®å»ºæ¨¡åçš„æŒ‡æ ‡æ•°å€¼ï¼Œè°ƒæ•´æ¨¡å‹ã€‚

- underfittingå’Œoverfitting
	
	R<sup>2</sup>å¤ªä½æˆ–å¤ªä½æˆ–å¤ªé«˜<br>
	å¤ªä½ç­‰äºå›å½’æ¨¡å‹æ²¡æœ‰æŠ“ä½æ ·æœ¬çš„ç‰¹å¾ï¼Œä¹Ÿå°±çº¦ç­‰äºæ‹Ÿåˆæ— æ•ˆ<br>
	å¤ªé«˜åˆ™æœ‰å¯èƒ½æ˜¯å› ä¸ºå¤ªè´´åˆæ ·æœ¬çš„ç‰¹å¾ï¼Œæ‰€ä»¥åè€Œæ— æ³•å»¶å±•å‡ºæ•´ä½“çš„ç‰¹å¾ï¼Œä¸èƒ½å¾ˆå¥½åœ°ç”¨äºé¢„æµ‹æœªçŸ¥æ•°æ®ç»„
	
- Adjusted R-squared value

	R<sup>2</sup>ä¼šéšç€æ ·æœ¬æ•°é‡å¢åŠ è€Œè‡ªç„¶è¶‹è¿‘äº1ï¼ŒAjusted R<sup>2</sup>å»é™¤äº†æ ·æœ¬æ•°é‡å’Œç‰¹å¾ï¼ˆè‡ªå˜é‡ï¼‰æ•°é‡å¯¹è¯„åˆ†çš„å½±å“ï¼Œæ‰€ä»¥æ›´å¥½ç”¨ã€‚

- å¸¸è§ç­›é€‰æ–¹æ³•
	- forward selection & backward elimiation
	
		forwardæ˜¯ä»ç¬¬ä¸€ä¸ªå¯èƒ½çš„ç‰¹å¾/å› ç´ /è‡ªå˜é‡å¼€å§‹ï¼Œä¸€ä¸ªä¸€ä¸ªåˆ¤æ–­æ˜¯å¦è¦åŒ…å«<br>
		backwardæ˜¯å…ˆåŒ…å«æ‰€æœ‰å¯èƒ½çš„ç‰¹å¾ï¼Œå†ä»æœ€åä¸€ä¸ªå¼€å§‹åˆ¤æ–­æ˜¯ä¸æ˜¯è¦å‰”é™¤

	- based on Extra-sum-of-squares F-test
		
		based on p-value

- Regularization æ­£åˆ™åŒ–
	
	è§£å†³è¿‡åº¦æ‹Ÿåˆçš„æ¨¡å‹ï¼Œé™ä½varianceå¢åŠ bias<br>
	Lasso Regression å»æ‰æ‰€æœ‰å¯¹é¢„æµ‹Yä¸å¤ªæœ‰ç”¨çš„ç‰¹å¾<br>
	Ridge Regression é™ä½ä¸é‡è¦ç‰¹å¾çš„å½±å“ä½†ä¸ä¼šå»æ‰å®ƒä»¬
	
	Elastic Net regression æµ‹è¯•lassoå’Œridgeå“ªä¸ªæˆ–æ··åˆæ¨¡å¼æ›´å¥½
	
	Principal component analysis (PCA) é˜…è¯»ææ–™é‡Œçš„æ¦‚å¿µ

## M4 Advanced hypothesis testing å‡è®¾æ£€éªŒ
### chi-squared test Ï‡<sup>2</sup>å¡æ–¹æ£€éªŒ

ç”¨äºæ£€éªŒä¸åˆ†ç±»å˜é‡ç›¸å…³çš„å‡è®¾

- Ï‡<sup>2</sup> Goodness of fit test å¡æ–¹é€‚åˆåº¦æ£€éªŒ

	è§‚æµ‹å€¼æ˜¯å¦ç¬¦åˆé¢„æœŸåˆ†å¸ƒè§„å¾‹<br>
	Null hypothesis(H<sub>0</sub>) è§‚æµ‹å€¼ç¬¦åˆé¢„æœŸåˆ†å¸ƒè§„å¾‹<br>
	Alternative hypothesis(H<sub>1</sub>) ä¸ç¬¦åˆé¢„æœŸåˆ†å¸ƒ
	
	å¡æ–¹å€¼è®¡ç®—å…¬å¼<br>
	Ï‡<sup>2</sup> = Î£((observed-expected)<sup>2</sup>/expedted)
	
	è‡ªç”±åº¦ï¼Œåˆ†ç±»æ•°-1
	
	å†ç»§ç»­è‹¥å¹²æ­¥è®¡ç®—åæŸ¥å‡ºP-valueï¼Œæ ¹æ®ç½®ä¿¡åº¦å†³å®šæ‹’ç»æˆ–æ¥å—H<sub>0</sub><br>
	P<ç½®ä¿¡åº¦ï¼Œæ‹’ç»H<sub>0</sub>
	
	expected valuesåˆ†ç±»æ•°å°äº5æ—¶ï¼Œé€‚åˆåº¦æ£€éªŒå¯èƒ½ä¸å‡†ç¡®ã€‚
	
	```python
	import scipy.stats as stats
	observations = [650, 570, 420, 480, 510, 380, 490]
	expectations = [500, 500, 500, 500, 500, 500, 500]
	result = stats.chisquare(f_obs=observations, f_exp=expectations)
	```
	
- Ï‡<sup>2</sup> Test for independence å¡æ–¹ç‹¬ç«‹æ€§æ£€éªŒ

	æ£€éªŒä¸¤ä¸ªåˆ†ç±»å˜é‡ä¹‹é—´æ˜¯å¦ç›¸äº’ç‹¬ç«‹<br>
	Null hypothesis(H<sub>0</sub>) ä¸¤ä¸ªåˆ†ç±»å˜é‡ä¹‹é—´ç›¸äº’ç‹¬ç«‹<br>
	Alternative hypothesis(H<sub>1</sub>) ä¸¤ä¸ªåˆ†ç±»å˜é‡äº’ç›¸å½±å“
	
	çŸ©é˜µï¼Œæ¨ªåˆ—æ˜¯å˜é‡1çš„è‹¥å¹²ç§æƒ…å†µï¼Œçºµåˆ—æ˜¯å˜é‡2çš„è‹¥å¹²ç§æƒ…å†µ<br>
	æ¨ªçºµäº¤å‰ç‚¹ä¸Šè®°å½•è§‚æµ‹æ•°é‡çš„ç´¯è®¡å€¼<br>
	å†è®¡ç®—æ¯ä¸ªäº¤å‰ç‚¹çš„æœŸæœ›å€¼E<sub>ij</sub> = R<sub>i</sub>_sum*C<sub>j</sub>_sum/Total
	
	å¡æ–¹å€¼è®¡ç®—åŒä¸Š<br>
	è‡ªç”±åº¦ï¼Œæ¨ªåˆ†ç±»æ•°mï¼Œçºµåˆ†ç±»æ•°nï¼Œ(m-1)*(n-1)<br>
	P-valueåˆ¤æ–­æ‹’ç»æˆ–æ¥å—å‡è®¾åŒä¸Š
	
	```python
	import numpy as np
	import scipy.stats as stats
	observations = np.array([[850, 450],[1300, 900]])
	result = stats.contingency.chi2_contingency(observations, correction=False)
	# result output order: the ğ›¸2 statistic, p-value, degrees of freedom, and expected values in array format
	```
	
### ANOVA æ–¹å·®åˆ†æ

ç”¨äºåˆ†æä¸€å¯¹å˜é‡ï¼ˆä¹‹ä¸­æœ‰ä¸€ä¸ªæ˜¯åˆ†ç±»å˜é‡ï¼‰ä¹‹é—´æ˜¯å¦äº’ç›¸å½±å“ã€‚<br>
æ–¹å·®åˆ†ææ¯”è¾ƒçš„æ˜¯åœ¨ä¸åŒåˆ†ç±»ä¸‹çš„æ•°æ®å¹³å‡æ•°å’Œæ•´ä½“æ•°æ®çš„å¹³å‡æ•°ã€‚

- One-way ANOVA

	ç”¨äºæ¯”è¾ƒ1ä¸ªåˆ†ç±»å˜é‡<br>
	é¦–å…ˆå°†æ ·æœ¬æ•°æ®æŒ‰å„ä¸ªç±»åˆ«åˆ†ç»„[[catogoricalX,numericalX]]<br>
	H<sub>0</sub>ï¼šåˆ†ç»„åï¼ŒnXçš„å¹³å‡å€¼åº”è¯¥ç›¸ç­‰ã€‚åˆ†ç±»å˜é‡å¯¹nXæ²¡æœ‰å½±å“ã€‚<br>
	H<sub>1</sub>ï¼šåˆ†ç»„åï¼Œè‡³å°‘æœ‰ä¸€ç»„nXçš„å¹³å‡å€¼ä¸å…¶ä»–ä¸åŒã€‚åˆ†ç±»å˜é‡å¯¹nXæœ‰å½±å“ã€‚<br>
	5ä¸ªæ­¥éª¤ï¼š
	- 1 è®¡ç®—æ¯ç»„/æ¯å„åˆ†ç±»ä¸‹çš„Yçš„ç»„å¹³å‡å€¼M<sub>g</sub>ï¼Œä»¥åŠæ‰€æœ‰Yçš„æ€»å¹³å‡å€¼M<sub>G</sub>
	- 2 è®¡ç®—SSBå’ŒSSW
		
		SSB = âˆ‘n<sub>g</sub> * (M<sub>g</sub>-M<sub>G</sub>)<sup>2</sup><br>		
		nï¼šæ¯ç»„æœ‰å¤šå°‘ä¸ªæ ·æœ¬
		
		SSW = âˆ‘âˆ‘(nX<sub>g</sub><sub>i</sub>-M<sub>g</sub>)<sup>2</sup><br>
		å…ˆè®¡ç®—æ¯ç»„Yä¸ç»„å¹³å‡ä¹‹é—´çš„å·®é¢å¹³æ–¹ï¼Œå†æ±‡æ€»å„ä¸ªç»„çš„å·®é¢å¹³æ–¹
	- 3 è®¡ç®—MSSBå’ŒMSSW
		
		MSSB = SSB/(k-1)<br>
		kä»£è¡¨æœ‰å¤šå°‘ç§åˆ†ç±»ï¼Œå¤šå°‘ç»„ï¼›k-1åŒæ—¶æ˜¯ç»„å†…è‡ªç”±åº¦
	
		MSSW = SSW/(n-k)<br>
		næ˜¯æ‰€æœ‰æ ·æœ¬çš„æ•°é‡ï¼Œå„ä¸ªç»„æ€»å’Œï¼›n-kåŒæ—¶æ˜¯ç»„é—´è‡ªç”±åº¦
	
	- 4 è®¡ç®—Få€¼
	
		F = MSSB/MSSW<br>
		Få€¼è¶Šå¤§ï¼Œè¶Šèƒ½è¡¨ç¤ºè‡³å°‘æœ‰ä¸€ç»„æ•°æ®å¯¹nXäº§ç”Ÿäº†å½±å“
		
	- 5 æŸ¥è¡¨ç¡®å®šp-valueï¼Œå¾—å‡ºæ˜¯å¦æ¨ç¿»nullå‡è®¾çš„ç»“è®º
	
	```python
	# å¯¼å…¥åº“
	import pandas as pd
	import seaborn as sns
	diamonds = sns.load_dataset("diamonds")
	diamonds = pd.read_cvs("diamonds.csv")
	# ç”Ÿæˆboxå›¾ï¼Œè‚‰çœ¼æŸ¥çœ‹ä¸¤ä¸ªå˜é‡ä¹‹é—´çš„å…³ç³»
	sns.boxplot(x = "color", y="log_price", data = diamonds)
	# å»ºç«‹å›å½’æ¨¡å‹è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
	import statsmodels.api as sm
	from statesmodes.formula.api import as ols
	model = ols(formula = "log_price ~ C(color)", data = diamonds).fit()
	# è¾“å‡ºstatistics ç»Ÿè®¡æŒ‡æ ‡
	model.summary()
	# è·‘ä¸€ä¸ªone-way ANOVA çœ‹Få€¼å’ŒP-value
	sm.stats.anova_lm(model, typ = 2)
	```

- Two-way ANOVA

	ç”¨äºæ¯”è¾ƒ2ä¸ªåˆ†ç±»å˜é‡
	
	|H<sub>0</sub>|H<sub>1</sub>
	|:------------|:------------
	|X1çš„ç±»åˆ«å¯¹Yæ²¡æœ‰å½±å“|X1çš„ç±»åˆ«å¯¹Yæœ‰å½±å“
	|X2çš„ç±»åˆ«å¯¹Yæ²¡æœ‰å½±å“|X2çš„ç±»åˆ«å¯¹Yæœ‰å½±å“
	|X1çš„ç±»åˆ«å¯¹Yçš„å½±å“ä¸X2çš„ç±»åˆ«å¯¹Yçš„å½±å“æ— å…³|X1ä¸X2çš„åˆ†ç±»ä¹‹é—´æœ‰äº’ç›¸ä½œç”¨å¹¶å½±å“Yå€¼
	
	```python
	# å¯¼å…¥æ•°æ®
	diamonds2 = pd.read_csv("diamonds2.csv")
	diamonds2.head()
	model2 = ols(formula = "log_price ~ C(color) + C(cut) + C(color):C(cut)", data = diamonds2).fit()
	sm.stats.anova_lm(model2, type = 2)
	# ä¸Šé¢çš„ç»“æœæœ‰å››è¡Œï¼Œå…¶ä¸­P-valueç”¨äºé€æ¡æ‹’ç» H0
	```
	
- post-hoc tests äº‹åæ£€éªŒ
	
	ç”±äºANOVAåªèƒ½å‘Šè¯‰æˆ‘ä»¬åˆ†ç±»å˜é‡å¯¹ç»“æœæœ‰å½±å“ï¼Œä½†å¹¶ä¸èƒ½å‡†ç¡®æŒ‡å‡ºæ˜¯å“ªï¼ˆäº›ï¼‰ç§åˆ†ç±»ä¼šå¸¦æ¥å½±å“ï¼Œè¿™æ—¶å€™éœ€è¦å†è¿›ä¸€æ­¥åšæ£€éªŒã€‚
	
	H<sub>0</sub>ï¼šä¸¤ç§åˆ†ç±»ä¸‹Yå€¼æ˜¯ä¸€æ ·çš„<br>
	H<sub>1</sub>ï¼šä¸¤ç§åˆ†ç±»ä¸‹çš„Yå€¼ä¸ä¸€æ ·
	
	Tukey's HSDæ³•ï¼Œæ›´æ³¨é‡ä¸è¦å‡ºç°typeIé”™è¯¯ï¼Œé”™è¯¯çš„å°†ä¸€å¯¹æœ¬æ¥æ˜¯æ²¡æœ‰å·®åˆ«çš„åˆ†ç±»è®¤åšæ˜¯æœ‰å·®åˆ«çš„ã€‚
	
	```python
	from statesmodes.status.multicom import pairwise_tukeyhsd
	tuke_oneway = pairwise_tukeyhsd(endog = diamonds["log_price"], groups = diamonds["color"], alpha = 0.05)
	tuke_oneway.summary()
	```
	
	ä¼šå¾—åˆ°ä¸€ä¸ªè¡¨æ ¼ï¼Œæœ€åä¸€åˆ—ä¼šç›´æ¥å†™å‡ºfalse or trueï¼Œä¸èƒ½æˆ–å¯ä»¥æ‹’ç»H<sub>0</sub>

### ANCOVA, MANNOVA and MANCOVA
- ANCOVA åæ–¹å·®åˆ†æ

	åˆ†æå¸¦æœ‰æœ‰å…±åŒä½œç”¨çš„ä¸¤ç»„æˆ–å¤šç»„åˆ†ç±»å˜é‡ã€‚ä½†ä¼šæ¶ˆé™¤ä¸¤ä¸ªåˆ†ç±»å˜é‡ä¹‹é—´çš„äº’ç›¸å½±å“ã€‚éš”ç¦»ä¸€ä¸ªå˜é‡ï¼Œåˆ†æå¦ä¸€ä¸ªåˆ†ç±»å˜é‡æ˜¯å¦å¸¦æ¥å½±å“ã€‚
	
	H<sub>0</sub>ï¼šä¸è®ºç¬¬ä¸€ä¸ªåˆ†ç±»å˜é‡å¦‚ä½•ï¼Œç¬¬äºŒä¸ªåˆ†ç±»å˜é‡ä¸ä¼šå¯¹Yé€ æˆå½±å“ã€‚Meanä¸€æ ·
	H<sub>1</sub>ï¼šä¸è®ºç¬¬ä¸€ä¸ªåˆ†ç±»å˜é‡å¦‚ä½•ï¼Œç¬¬äºŒä¸ªåˆ†ç±»å˜é‡ä¼šå¯¹Yé€ æˆå½±å“ã€‚Meanä¸ä¸€æ ·
	
- MANOVA & MANCOVA

	åœ¨ANOVAå’ŒANCOVAåŸºç¡€ä¸Šï¼Œå¢åŠ äº†åˆ†æåˆ†ç±»å˜é‡å¯¹å¤šä¸ªè¿ç»­çš„outcome variablesçš„å½±å“ã€‚
	
	(è¿˜éœ€è¦å†ä»”ç»†äº†è§£ä»¥ä¸Šå››ç§å›å½’åˆ†ææ–¹æ³•åˆ°åº•å„è‡ªæ˜¯åšä»€ä¹ˆç”¨çš„)

## Logisic regression é€»è¾‘å›å½’
### Foundations of logistic regression é€»è¾‘å›å½’åŸºç¡€
- Binomial logistic regression äºŒå…ƒé€»è¾‘å›å½’
	
	å½“Yæ˜¯æœ‰ä¸¤ä¸ªé€‰é¡¹çš„åˆ†ç±»å˜é‡æ—¶ï¼Œæ‹Ÿåˆè½å…¥2é€‰1ç»“æœçš„æ¦‚ç‡ã€‚

- é€»è¾‘å‡è®¾
	- Linearity assumption
		
		Xå’Œlogit(Y=1)çº¿æ€§ç›¸å…³
		
		odds = p(Y=1)/(1-p(Y=1))<br>
		logit >> log it >> log odds<br>
		logit(Y=1)=log(p(Y=1)/(1-p(Y=1)))
		
		log(p/(1-p)) = Î²<sub>0</sub>+Î²<sub>1</sub>X<sub>1</sub>+...+Î²<sub>n</sub>X<sub>n</sub>
	
	- Independent observations æ ·æœ¬ä¹‹é—´ç›¸äº’ç‹¬ç«‹
	- No multicollinerity å„ä¸ªXä¹‹é—´æ— çº¿æ€§å…³ç³»
	- No extreme outliers æ²¡æœ‰æç«¯æ ·ä¾‹
		
		è¿™ä¸ªå‡è®¾å’Œçº¿æ€§å›å½’ä¸åŒã€‚ä¸€èˆ¬åœ¨è·‘å®Œå›å½’æ¨¡å‹ä»¥åæ‰èƒ½åˆ¤æ–­æ ·æœ¬åç¦»ç¨‹åº¦ã€‚å¦‚æœæœ‰æç«¯æ ·ä¾‹ï¼Œå¯ä»¥è€ƒè™‘å¯¹æ ·æœ¬æ•°æ®è¿›è¡Œæ¢ç®—è°ƒæ•´ï¼Œæˆ–è€…å¹²è„†å»æ‰æç«¯æ ·æœ¬ã€‚
		
- MLE æœ€å¤§å¯èƒ½æ€§æ³•
	
	æ‰¾å‡ºè§‚å¯Ÿåˆ°æ‰€æœ‰æ ·æœ¬åŒæ—¶å‘ç”Ÿçš„æœ€å¤§å¯èƒ½æ€§ã€‚å› ä¸ºæ ·æœ¬ä¹‹é—´ç›¸äº’ç‹¬ç«‹ï¼Œæ‰€ä»¥è§‚å¯Ÿåˆ°ä¸€ç»„æ ·æœ¬çš„æ¦‚ç‡å°±æ˜¯æ¯ä¸ªæ ·æœ¬å•ç‹¬å‘ç”Ÿæ¦‚ç‡çš„ä¹˜ç§¯ã€‚
	
### Logistic regression with Python

	```python
	# è·å¾—ä¸€ç»„æ•°æ®çš„ç»Ÿè®¡å­¦ç‰¹å¾
	activity.describe()
	# é¢„è§ˆå‰å‡ è¡Œæ•°æ®
	activity.head()
	# å¯¼å…¥å¯è¿›è¡Œé€»è¾‘å›å½’åˆ†æçš„åº“
	from sklearn.model_selection import train_test_split
	from sklearn.linear_model import LogisicRegression
	# æŒ‡å®šXå’ŒY
	x = activity[["col_1"]]
	y = activity[["Category_col_2"]]
	# æŠŠæ•°æ®åˆ†æˆå»ºæ¨¡å’Œæµ‹è¯•ä¸¤éƒ¨åˆ†
	x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)
	# å»ºç«‹é€»è¾‘å›å½’æ¨¡å‹ï¼Œå¹¶å°†æ¨¡å‹èµ‹å€¼ç»™clfï¼Œclassifierçš„ç®€ç§°
	clf = LogisicRegression().fit(x_train,y_train)
	# è·å¾—Î²<sub>1</sub>
	clf.coef_
	# è·å¾—Î²<sub>0</sub>
	clf.intercept_
	# ç”»ä¸€ä¸ªæœ‰æ ·æœ¬å’Œå›å½’æ›²çº¿çš„å›¾
	import seaborn as sns
	sns.regplot(x="col_1", y="Category_col_2", data=activity, logistic=True) 
	```

### Interpret logistic regression results è§£é‡Š

	```python
	# ç”¨æµ‹è¯•æ•°æ®æ£€éªŒ
	y_pred = clf.predict(x_test)
	# predictä¼šå°†æ¦‚ç‡å¤§äºç­‰äº0.5çš„åŠ æ ‡ç­¾1
	# å¦‚æœæƒ³çœ‹æ¦‚ç‡åˆ°åº•æ˜¯ä»€ä¹ˆï¼Œç”¨predict_proba
	clf.predict_proba(x_test)[::,-1]
	```

- Confusion matrix æ··æ·†çŸ©é˜µ

	çœ‹é¢„æµ‹çš„åˆ†ç±»ä¸å®é™…åˆ†ç±»ç›¸æ¯”ï¼Œå‡†ç¡®åº¦å¦‚ä½•
	
	```python
	import sklearn.metrics as metrics
	cm = metrics.confusion_matrix(y_test, y_pred, labels=clf.classes_)
	disp = metrics.ConfusionMatrixDisplay(confusion_matrix=cm, display_labels = clf.classes_)
	# ç”»å››è±¡é™å›¾ï¼šä¸¤ç§é¢„æµ‹å¯¹äº†çš„ï¼Œå’ŒtypeI typeIIé”™è¯¯åˆ†åˆ«æ˜¾ç¤º
	disp.plot()
	# precision/recall/accuracy
	metrics.precision_score(y_test, y_pred)
	metrics.recall_score(y_test, y_pred)
	metrics.accuracy_score(y_test, y_pred)
	```
	
	- Precision ç²¾ç¡®ç‡/æŸ¥å‡†ç‡
		
		True positive/(True positives + False positives)
		
	- Recall å¬å›ç‡/æŸ¥å…¨ç‡
	
		True positives/(True positives + False negatives)
		
	- Accuracy å‡†ç¡®ç‡
		
		ï¼ˆTrue positives + False negatives)/Total predictions 

- ROC curve & AUC
	
	ROC curve receiver operating characteristic curveï¼Œå±•ç°True positive rate(recall) å’Œ False Negative rate ä¹‹é—´å¯¹åº”å…³ç³»ã€‚Xè½´ä¸ºFPï¼ŒYè½´ä¸ºTPã€‚
	
	ç†æƒ³çš„æ¨¡å‹æ˜¯åœ¨TPé«˜çš„åŒæ—¶FPä½ï¼Œæ‰€ä»¥ROCæ›²çº¿è¶Šå‡¸å‡ºæ¥è¿‘å·¦ä¸Šè§’ï¼Œæ¨¡å‹è¶Šå¥½ã€‚
	
	```python
	import matplotlib.pyplot as plt 
	from sklearn.metrics import RooCurveDisplay
	RocCurveDisplay.from_predictions(y_test, y_pred)
	plt.show()
	```
	
	AUC area under the ROC curveï¼ŒæŒ‡çš„æ˜¯ROCæ›²çº¿å’ŒXè½´ä¹‹é—´å½¢æˆçš„é¢ã€‚0~100%ã€‚AUC=0æ˜¯é¢„æµ‹å®Œå…¨é”™è¯¯ï¼ŒAUC=100%/1æ˜¯é¢„æµ‹å®Œå…¨æ­£ç¡®ã€‚ä¸€èˆ¬æ¥è¯´AUCä½äº0.5è¯´æ˜æ¨¡å‹é¢„æµ‹åˆ†ç±»çš„èƒ½åŠ›å’Œéšæœºåˆ†é…åˆ†ç±»æ²¡æœ‰åŒºåˆ«ã€‚
	
	```python
	metrics.roc_auc_score(y_test,y_pred)
	roc_auc_score()
	```
- è§£é‡Šé€»è¾‘å›å½’æ¨¡å‹
	- logit
		
		åœ¨è¿›è¡Œé€»è¾‘å›å½’æ—¶ï¼Œæˆ‘ä»¬è¿›è¡Œäº†å‡ æ¬¡æ€è·¯è½¬æ¢ï¼š<br>
		é¦–å…ˆï¼Œè§‚å¯Ÿè½å…¥æŸç§å½’ç±»çš„æ¦‚ç‡ï¼Œç»™åˆ†ç±»æ‰¾åˆ°ä¸€ç§å¯ä»¥ç”¨æ•°å­—æè¿°çš„æ–¹å¼ï¼›<br>
		ç„¶åè®¡ç®—å‡º æ¯ä¸ªæ ·æœ¬å¦‚æœæ­£å¥½æ˜¯è¦è¿›ä¸€æ­¥ç ”ç©¶çš„åˆ†ç±» è¿™æ ·çš„æ¦‚ç‡oddæ˜¯å¤šå°‘ï¼›<br>
		ç„¶åå¯¹oddåšä¸€æ¬¡å–å¯¹æ•°logè¿ç®—logit(Y=1)ï¼›<br>
		æœ€åï¼Œå‘ç°logitå’ŒXä¹‹é—´å­˜åœ¨çº¿æ€§å…³ç³»ï¼Œæ‰¾å‡ºçº¿æ€§å›å½’å…¬å¼
		
		æƒ³è¦ç”¨ä¸€èˆ¬çº¿æ€§å›å½’çš„æ€è·¯æ¥è§£é‡ŠXå’Œå½’ç±»Yä¹‹é—´çš„å…³ç³»ï¼Œå°±éœ€è¦åŠ å…¥é€†å‘æ¢ç®—ã€‚