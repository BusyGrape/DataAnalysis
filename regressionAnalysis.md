# Regression Analysis å›å½’åˆ†æ

ç®€å•ä»‹ç»åšâ€œå›å½’åˆ†æâ€çš„æ­¥éª¤PACEï¼šä»æ‹†è§£é—®é¢˜åˆ°ç»“è®ºé‡Šä¹‰<br>
å­¦ä¹ ä¸¤ç§å›å½’åˆ†ææ–¹æ³•linear and logisticï¼šçº¿æ€§å›å½’å’Œé€»è¾‘å›å½’<br>
äº†è§£è¿™ä¸¤ç§å›å½’åˆ†æå¦‚ä½•è§£å†³å®é™…é—®é¢˜ã€‚

## M1 Introduction
### å›å½’åˆ†æå››æ­¥æ³• PACE

P è®¡åˆ’ planâ€”â€”å……åˆ†äº†è§£é¡¹ç›®éœ€æ±‚å’Œå¯ç”¨èµ„æº<br>
A åˆ†æ Analyzeâ€”â€”è¯•è¯•å“ªä¸ªæ¨¡å‹æ›´é€‚åˆç”¨æ¥åšå›å½’åˆ†æ<br>
C å»ºæ¨¡ Constructâ€”â€”å»ºæ¨¡å¹¶å¤„ç†æ•°æ®<br>
E æ‰§è¡Œ Executeâ€”â€”æ ¹æ®è¿ç®—ç»“æœè¿›è¡Œåˆ†æï¼Œå¾—åˆ°ç»“è®ºï¼Œå¹¶è§£é‡Š

### Liner Regression çº¿æ€§å›å½’

é€‚åˆç”¨æ¥æè¿°ï¼š<br>
ä¸€ç§å› ç´ Yä¼šè·Ÿéšç€å¦ä¸€ç§å› ç´ Xçš„å˜åŒ–è€Œæ”¹å˜ã€‚ï¼ˆå½±å“å› ç´ Xå¯ä»¥ä¸æ­¢ä¸€ç§ï¼‰<br>
ä»–ä»¬ä¹‹é—´çš„ä¼´éšå…³ç³»æ˜¯çº¿æ€§çš„ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼ŒYä¼šæŒ‰æŸç§æ¯”ä¾‹ï¼ŒéšXçš„å¢åŠ è€Œå¢åŠ ï¼ˆæˆ–å‡å°‘ï¼‰

Î¼{Y|X} = Î²<sub>0</sub>+Î²<sub>1</sub>X

- Yï¼ŒDependent variable è¢«å½±å“å› ç´ 
- Xï¼ŒIndependent variable(s) å½±å“å› ç´ ï¼Œè‡ªå˜é‡
- Regression coefficients å›å½’æ–¹ç¨‹ç³»æ•°
	- Î²<sub>1</sub>ï¼ŒSlope æ–œç‡
	- Î²<sub>0</sub>ï¼ŒIntercept æˆªè·
- OLSï¼Œordinary least squares estimationï¼Œæ¨ç®—çº¿æ€§å›å½’å…¬å¼çš„æ–¹æ³•
	- loss functiion ç†æƒ³Yä¸å®é™…è§‚æµ‹åˆ°çš„Yä¹‹é—´çš„å·®

### Logistic Regression é€»è¾‘å›å½’

é€‚åˆç”¨æ¥æè¿°ï¼š<br>
éšç€æŸç§å› ç´ Xçš„å˜åŒ–ï¼ŒYçš„å½’ç±»ä¼šæ”¹å˜ã€‚ï¼ˆå½±å“å› ç´ Xå¯ä»¥ä¸æ­¢ä¸€ç§ï¼‰<br>
Yçš„å½’ç±»æ˜¯å¯ä»¥è®¡æ•°çš„ï¼Œä»ä¸¤ç±»åˆ°æœ‰é™å¤šç±»ã€‚

Î¼{Y|X} = Prob(Y = n|X) = p

- pï¼Œprobability å½“Xæ˜¯æŸä¸ªå€¼çš„æ—¶å€™ï¼ŒYå±äºç¬¬nç§å½’ç±»çš„å¯èƒ½æ€§æ˜¯å¤šå°‘ã€‚
- link function æ¨ç®—é€»è¾‘å›å½’å…¬ç¤ºçš„æ–¹æ³•

### Correlation is not Causation 

ä¸¤ç§å› ç´ ç›¸å…³ï¼Œå¹¶ä¸ç­‰äºè°èƒ½å¼•å‘è°ã€‚è¿™ä¸æ˜¯ä¸€ç§å› æœå…³ç³»ã€‚

## M2 Simple linear regression ä¸€å…ƒçº¿æ€§å›å½’æ¨¡å‹

Yçš„å˜åŒ–åªå—ä¸€ä¸ªå› ç´ Xå½±å“ã€‚<br>
åœ¨ä¸€ä¸ªäºŒç»´åæ ‡ç³»é‡Œï¼Œæ¯å¯¹ç›¸å…³å› ç´ Xå’ŒYçš„å…³ç³»èƒ½æŠ˜å°„æˆä¸€ä¸ªç‚¹ã€‚æ‰¾å‡ºä¸€æ¡ç›´çº¿ï¼ˆçš„æ–¹ç¨‹ï¼‰ï¼Œæœ€èƒ½è¡¨ç¤ºè¿™äº›ç‚¹çš„åˆ†å¸ƒè¶‹åŠ¿ã€‚

### OLS æœ€å°è¯¯å·®æ³•æ¨ç®—å›å½’æ–¹ç¨‹

minimizing loss function / error

- Îµï¼ŒResidual æ®‹å·®ï¼ŒYè§‚æµ‹å€¼ ä¸ Yçš„æ¨æµ‹å€¼ ä¹‹é—´çš„å·®å¼‚

	Î£(Îµ) == 0 ä½¿ç”¨OLSæ³•ï¼Œæ®‹å·®å’Œæ°¸è¿œä¸ºé›¶

- SSRï¼Œsum of squared residuals æ®‹å·®å¹³æ–¹å’Œ
- åœ¨æ‰€æœ‰çš„ç›´çº¿é€‰é¡¹é‡Œï¼Œæ‰¾é‚£æ¡è®©SSRå€¼æœ€ä½çš„ï¼ˆçº¿ï¼‰
	
### ç”¨rè®¡ç®—å›å½’çº¿æ–¹ç¨‹
- rï¼ŒPearson's correlation / linear correlation coefficient çº¿æ€§ç›¸å…³ç³»æ•°	

	- råœ¨[-1,1]ä¹‹é—´

		rçš„ç»å¯¹å€¼è¶Šå¤§ï¼Œæ ·æœ¬ç‚¹è¶Šåƒä¸€æ¡ç›´çº¿ï¼›rè¶Šå°ï¼Œæ ·æœ¬ç‚¹è¶Šæ¥è¿‘ä¸€å›¢æ— åºäº‘çŠ¶æ•£ç‚¹

	- r æ­£è´Ÿçš„å«ä¹‰

		r>0æ„å‘³ç€æ–œç‡æ˜¯æ­£æ•°ï¼Œå€¾æ–œè§’åº¦â†—<br>
		r<0æ„å‘³ç€æ–œç‡æ˜¯è´Ÿæ•°ï¼Œå€¾æ–œè§’åº¦â†˜

	- rçš„æ•°å€¼ä¸æ˜¯æ–œç‡
	- r = covariance(X,Y)åæ–¹å·®/(SD X)(SD Y)æ ‡å‡†å·®
	
- æœ‰ä¸¤æ¡å®šå¾‹
	- Xçš„å¹³å‡å€¼å’ŒYçš„å¹³å‡å€¼æ°¸è¿œä¼šè½åœ¨å›å½’çº¿ä¸Š
	- å¦‚æœXå¢åŠ 1ä¸ªXæ ‡å‡†å·®ï¼Œé‚£ä¹ˆYä¼šå¢åŠ rä¸ªYæ ‡å‡†å·®
	
- å›å½’çº¿çš„æ–œç‡Î²<sub>1</sub> = r(SD Y)/SD X
		
### ç”¨Pythonæ¥å®Œæˆæ‰€æœ‰è®¡ç®—
- A é¢„åˆ†æé˜¶æ®µ
	- è§‚å¯Ÿä¸¤ä¸¤æ•£ç‚¹å›¾çŸ©é˜µ
			
		```python
		# æˆ‘æœäº†ä¸€ä¸‹ï¼Œå¤šä¸ªå¸–å­éƒ½è¯´
		# ç”»ä¸¤ä¸¤æ•£ç‚¹å›¾å°±ç”¨seabornåº“
		import seaborn as sns		
		# ç»™æ¯ä¸¤ä¸ªå˜é‡ä¹‹é—´ç”»ä¸€å¹…æ•£ç‚¹å›¾
		sns.pairplot(origData)
		```

- C å»ºæ¨¡
	- Step1 Build a model
			
		```python
		# Subset Data æ¸…æ´—å¹¶é€‰æ‹©è¦è¿›è¡Œå›å½’åˆ†æçš„ä¸¤åˆ—æ•°æ®
		ols_data = origData[["column1/X", "column2/Y"]]
		# Write out formula å®šä¹‰Yå’ŒXåˆ†åˆ«æ˜¯å“ªåˆ—æ•°æ®
		ols_formula = "column2/Y ~ column1/X"
		# Import ols function
		from statsmodels.formula.api import ols
		# Build OLS, fit model to data ç”¨OLSæ–¹æ³•å»ºæ¨¡è®¡ç®—å‡ºå›å½’çº¿
		OLS = ols(formula = ols_formula, data = ols_data)
		model = OLS.fit()
		```
							
	- Step 2 Model evaluation

		P-valueï¼ŒConfidence Intervals

		```python
		# print statistics è¾“å‡ºæ¨¡å‹çš„å„é¡¹ç»Ÿè®¡æŒ‡æ ‡
		model.summary()

		# confidence band 
		sns.regplot(x="column1/X", y="column2/Y", data = ols_data)
		
		# Xï¼Œå–Xå€¼
		X = ols_data["column1/X"]
		# Yï¼Œç”¨é¢„æµ‹å…¬å¼Predictè·å¾—Yå€¼fitted_values
		fitted_values = model.predict(X)
		# Residualsï¼Œç”¨residå…¬å¼è·å¾—æ®‹å·®å€¼
		residuals = model.resid
		```
		
		Homoscedasticity

		```python
		# Residualsåœ¨0é™„è¿‘çš„åç§»é‡æ•£ç‚¹å›¾
		import matplotlib.pyplot as plt
		fig = sns.scatterplot(x=fitted_values, y=residuals)
		fig.axhline(0)
		fig.set_xlabel("Fitted Values")
		fig.set_ylabel("Residuals")
		plt.show()
		```
		
		Normality

		```python
		# Residualsçš„æŸ±çŠ¶å›¾
		fig = sns.histplot(residuals)
		fig.set_xlabel("Residual Value")
		fig.set_title("Histogram of Residuals")
		plt.show()
		# Q-Q plotå›¾	
		import statsmodels.api as sm
		fig = sm.qqplot(model.resid, line = 's')
		plt.show()
		```
			
		R<up>2</up>ï¼ŒMSE/MAE
		```python
		# å¯¼å…¥åº“sklearn
		from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
		# å‡è®¾å·²ç»æœ‰ä¸€ç»„Hold_out sample[["x_test","y_test"]]
		y_test_pred = model.predict(x_test)
		MSE = mean_squared_error(y_test,y_test_pred)
		MAE = mean_absolute_error(y_test,y_test_pred)
		R2 = r2_score(y_test,y_test_pred)
		print('MSE:',MSE,'/nMAE:',MAE,'/nR2:',R2)
		```		
		
- E å¯ä»¥ç”¨pythonæ¥å®ŒæˆData visualisationå·¥ä½œ
		
### å»ºæ¨¡åˆ†æè¿‡ç¨‹PACE

- A æ£€æŸ¥çº¿æ€§å›å½’å‡è®¾æ˜¯ä¸æ˜¯éƒ½æ»¡è¶³
	- Linearity ä¸¤ä¸ªå˜é‡XYä¹‹é—´æ˜¯å¦æ˜¯çº¿æ€§ç›¸å…³

		å¯ä»¥å…ˆæŠ“æ ·æœ¬ç”»ä¸ªæ•£ç‚¹å›¾ï¼Œçœ‹çœ‹è¿™äº›ç‚¹çš„åˆ†å¸ƒåƒä¸åƒç›´çº¿
	
	- Normality æ®‹å·®residualæ˜¯å¦æ˜¯normal distributedæ­£æ€åˆ†å¸ƒ

		å¿…é¡»è¦å»ºå®Œæ¨¡å‹åæ‰èƒ½æ£€éªŒï¼Œå¯ä»¥ç›´æ¥ç”»æŸ±çŠ¶å›¾ä¹Ÿå¯ä»¥ç”»Q-Q plotå›¾æ¥è§‚å¯Ÿåˆ¤æ–­

	- Independent observations é‡‡é›†çš„æ ·æœ¬ä¹‹é—´æ²¡æœ‰ç›¸äº’å½±å“
		ä¸»è¦é åˆ†ææ•°æ®é‡‡é›†æ­¥éª¤æ¥åˆ¤æ–­ï¼Œæˆ–è€…å»ºæ¨¡åè¾“å‡ºç»“æœæ˜¾ç¤ºæœ‰é—®é¢˜æ—¶èƒ½å‘ç°æ ·æœ¬ä¸ç¬¦åˆå‡è®¾
	
	- Homoscedasticity æ®‹å·®çš„åç§»é‡æŒç»­ä¸”å­˜åœ¨ä¸”æ•°å€¼éšæœºä½†æ¥è¿‘

		å¿…é¡»è¦å»ºå®Œæ¨¡å‹åæ‰èƒ½æ£€éªŒï¼ŒåŒæ ·å¯ä»¥é€šè¿‡ç”»å‡ºæ•£ç‚¹å›¾æ¥è§‚å¯Ÿåˆ¤æ–­
		
- C ç”¨é€‚åˆçš„å˜é‡å»ºæ¨¡ï¼Œå¾—åˆ°å„é¡¹ç»Ÿè®¡æ•°æ®
	- å»ºæ¨¡ï¼ˆå‚è§ï¼šç”¨Pythonæ¥å®Œæˆæ‰€æœ‰è®¡ç®—/C å»ºæ¨¡ï¼‰
	- è¯„ä¼°
		
		äº†è§£summaryç»™å‡ºçš„å„ä¸ªæŒ‡æ ‡éƒ½æ˜¯ä»€ä¹ˆæ„æ€
		- confidence band
		
			åœ¨å›å½’çº¿é™„è¿‘ï¼Œè½åœ¨ç½®ä¿¡åŒºé—´é‡Œæ‰€æœ‰çš„ç›´çº¿ç»„æˆçš„ä¸€ç‰‡åŒºåŸŸ<br>
			ç”¨æ ·æœ¬è¿›è¡Œå›å½’åˆ†æï¼Œæ€»æ˜¯å¯èƒ½ä¸æ•´ä½“çš„å›å½’åˆ†æç»“æœå­˜åœ¨åå·®
		
		- P-value			
		
			null hypothesisï¼ˆXYä¸å­˜åœ¨çº¿æ€§ç›¸å…³æ€§ï¼‰æˆç«‹çš„å¯èƒ½æ€§ï¼ˆæ¦‚ç‡ï¼‰<br>
			å¦‚æœPå°äºç½®ä¿¡åº¦ï¼ˆ5%ï¼‰ï¼Œnull hypothesisè¢«æ¨ç¿»<br>
			å¯ä»¥è®¤ä¸ºXYä¹‹é—´çº¿æ€§ç›¸å…³ï¼Œcoefficientä¸æ˜¯0
			
		- Confidence Intervals [0.025, 0.975]
		
			è¡¨ç¤ºæœ‰5%çš„æœºä¼šï¼Œæ–œç‡å’Œæˆªè·çš„ç½®ä¿¡èŒƒå›´å€¼ä¸èƒ½åŒ…å«å›å½’çº¿çš„çœŸå®å‚æ•°<br>
			åœ¨è¿™ä¸€åˆ—ä¸‹é¢ç»™å‡ºæ¥çš„æ•°æ®æ˜¯æˆªè·å’Œæ–œç‡çš„èŒƒå›´ï¼Œç”¨æ¥ç”»å‡ºcofidence bandåŒºåŸŸ
		
		å¸¸è§çš„è¯„ä¼°çŸ©é˜µ
		- R<sup>2</sup> å†³å®šç³»æ•° 0~1ä¹‹é—´
			
			ç”¨æ¥æè¿°Xå¯¹Yçš„å½±å“ç¨‹åº¦ã€‚è¶Šæ¥è¿‘äº1è¯´æ˜è¶Šé€‚åˆç”¨çº¿æ€§å›å½’åˆ†æã€‚
			çº¿æ€§ç›¸å…³ç³»æ•°rçš„å¹³æ–¹(æœ‰å¾…éªŒè¯ï¼Œè®¡ç®—å…¬å¼ä¸ä¸€æ ·)
			
		- MSEï¼Œmean squared error
			
			å¯¹outlieræ•æ„Ÿï¼Œå€¼è¶Šå°è¶Šå¥½
			
		- MAEï¼Œmean absolute error 
			
			åœ¨æœ‰outlieræ—¶ä½¿ç”¨ï¼Œå€¼è¶Šå°è¶Šå¥½ 
			
		- Hold-out sample 
			
			ä¸èƒ½æ˜¯ä¹‹å‰å»ºæ¨¡fit modelæ—¶ä½¿ç”¨è¿‡çš„æ•°æ®ï¼Œå¯ä»¥ç”¨è¿™ç»„æ–°æ•°æ®æ£€æµ‹ä»¥ä¸Šä¸‰ä¸ªå€¼
			
- E å¯¹å»ºæ¨¡ç»“æœè¿›è¡Œå…¨é¢è¯„ä¼°å’Œè§£é‡Š
	- è§£é‡Šæ¨¡å‹çš„ç»Ÿè®¡æŒ‡æ ‡éƒ½æ„å‘³ç€ä»€ä¹ˆï¼Œæ¯”å¦‚æ–œç‡è¡¨æ˜çš„Yå°†å¦‚ä½•å› ä¸ºXè€Œå˜åŒ–
	- å°½é‡å°†æ•°å­—è½¬åŒ–æˆæ˜“äºç†è§£çš„å›¾åƒã€åŠ¨ç”»ç­‰æ¥è¿›è¡Œè®²è§£æˆ–æ¼”ç¤º
	- æœ‰å¿…è¦æé†’å¬ä¼—æ¨¡å‹ä»å¯èƒ½åœ¨ä»€ä¹ˆæƒ…å†µä¸‹å¤±æ•ˆï¼Œæˆ–éœ€è¦è¿›è¡Œä¿®æ­£
	- ä½†è¦å°½é‡é¿å…ä½¿ç”¨æ™¦æ¶©çš„æœ¯è¯­ï¼Œå¦‚coefficients or P-value 
	- è¦æ³¨æ„åŒºåˆ†correlationå’Œcausationçš„åŒºåˆ«ï¼Œæˆ‘ä»¬å‡ ä¹æ— æ³•åœ¨è¿™é‡Œè¯æ˜causaiton

## M3 Multiple linear regression å¤šå…ƒçº¿æ€§å›å½’æ¨¡å‹

å½“ä¸æ­¢ä¸€ä¸ªå› ç´ å…±åŒå½±å“Yæ—¶ï¼Œå¼•å…¥å¤šå…ƒçº¿æ€§å›å½’æ¨¡å‹

Y = Î²<sub>0</sub>+Î²<sub>1</sub>X<sub>1</sub>+...+Î²<sub>n</sub>X<sub>n</sub>

### One hot encoding ç‹¬çƒ­ç¼–ç 

å¦‚æœå½±å“å› ç´ X<sub>i</sub>æ˜¯åˆ†ç±»å˜é‡ã€‚ç±»ä¼¼å¤šé€‰é¢˜çš„ç­”æ¡ˆï¼Œå¦‚ï¼šåšæˆ–è€…ä¸åšï¼Œé€‰æ‹©äº†ACæ²¡æœ‰é€‰BDEã€‚<br>
è¿™æ—¶ä¸ºäº†å¯ä»¥è¿›è¡Œå›å½’åˆ†æï¼Œè¦æŠŠX<sub>i</sub>æ‹†è§£æˆä¸€ç»„äºŒè¿›åˆ¶æ•°æ¥è¡¨ç¤ºå®ƒçš„å…¨éƒ¨ç‰¹å¾<br>
X<sub>i</sub>â†’X<sub>iA</sub>,X<sub>iB</sub>,...,X<sub>iN</sub><br>
äºŒè¿›åˆ¶çš„ä½æ•°å’ŒXæœ‰å‡ ä¸ªç‰¹å¾æœ‰å…³

### Interaction äº¤äº’é¡¹/äº¤å‰å˜é‡

è¿™ä¸ªæ¦‚å¿µæˆ‘è§‰å¾—æœ¬è¯¾è¯´çš„ä¸å¾ˆæ¸…æ¥šï¼Œçœ‹å®Œåæˆ‘è¿˜æœ‰å¥½å‡ ä¸ªç–‘é—®<br>
ä»€ä¹ˆæ—¶å€™åŠ å…¥äº¤äº’é¡¹ï¼Ÿä¸è®ºå½±å“å› ç´ æ˜¯åˆ†ç±»å˜é‡è¿˜æ˜¯è¿ç»­æ•°å€¼å˜é‡éƒ½å¯ä»¥å¼•å…¥äº¤äº’é¡¹ä¹ˆï¼Ÿå¦‚ä½•è§£è¯»äº¤äº’é¡¹ï¼Ÿ
æœ€åæ‰¾äº†ä¸€ç¯‡çŸ¥ä¹æ–‡ç« ç®—æ˜¯åŸºæœ¬çœ‹æ˜ç™½äº†ï¼Œé…åˆè¯„è®ºåŒºå°±æ›´å…¨é¢çš„å›ç­”äº†æˆ‘çš„ç–‘é—®ã€‚

[ä¸€æ–‡è½»æ¾çœ‹æ‡‚äº¤äº’ä½œç”¨](https://zhuanlan.zhihu.com/p/224990519 "")

### å¤šå…ƒçº¿æ€§å›å½’å‡è®¾
- Linearity 
	
	çœ‹æ¯ä¸€ä¸ªX<sub>i</sub>å’ŒYçš„æ•£ç‚¹å›¾æ˜¯ä¸æ˜¯åƒä¸€æ¡çº¿
	
- Independance, Normality, Homoscedasticity
	
	å®šä¹‰å’Œæ£€æµ‹éƒ½å’Œä¸€å…ƒçº¿æ€§å›å½’å‡è®¾ä¸€æ ·
	
- No multicollinerity assumption è‡ªå˜é‡ä¹‹é—´æ²¡æœ‰çº¿æ€§å…³ç³»å‡è®¾

	é€šè¿‡æ‰€æœ‰å˜é‡ä¹‹é—´çš„ä¸¤ä¸¤æ•£ç‚¹å›¾æ¥åˆ¤æ–­

	```python
	sns.pairplot()
	```
	å¦‚æœæ•£ç‚¹å›¾ä¸å¥½åˆ¤æ–­ï¼Œå¯ä»¥è®¡ç®—ä¸¤ä¸ªå˜é‡ä¹‹é—´çš„VIFå€¼ï¼ˆ1~âˆï¼‰ã€‚VIFè¶Šå¤§çº¿æ€§å…³ç³»è¶Šå¼ºã€‚
	
	```python
	from statsmodels.stats.outliers_influence import variance_inflation_factor
	X = df[['col_1', 'col_2', 'col_3']]
	vif = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]
	vif = zip(X, vif)
	print(list(vif))
	```
	é¿å…åŒæ—¶æŒ‘é€‰ä¸Šä¸¤ä¸ªæ˜æ˜¾æœ‰çº¿æ€§å…³ç³»çš„å˜é‡ä½œä¸ºX<sub>i</sub>&X<sub>j</sub>ï¼Œæˆ–æ˜¯å°†ä¸¤ä¸ªæœ‰å¾ˆå¼ºçº¿æ€§å…³ç³»çš„å˜é‡è½¬åŒ–æˆä¸€ä¸ªæ–°çš„å˜é‡ã€‚
		
### ç”¨pythonå»ºç«‹å¤šå…ƒå›å½’æ¨¡å‹
- C å»ºæ¨¡

	```python
	# å‡†å¤‡æ•°æ®
	X = origData[["col_1/X1","col_2/X2",...,"col_n/Xn"]]
	Y = origData[["col_0/Y"]]
	# å¯¼å…¥åº“
	from sklearn.model_selection import train_test_split
	# æŠŠæ•°æ®åˆ†æˆå»ºæ¨¡å’Œæµ‹è¯•ä¸¤éƒ¨åˆ†
	X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.3, random_state = 42]
	# å‡†å¤‡å»ºæ¨¡ç”¨api
	ols_data = pd.concat([X_train, y_train], axis = 1)
	# Write out formula å®šä¹‰Yå’ŒXåˆ†åˆ«æ˜¯å“ªåˆ—æ•°æ®
	ols_formula = "col_0/Y ~ col_1/X1 + C(col_2/categorical X2)+...+col_n/Xn"
	# Import ols function
	from statsmodels.formula.api import ols
	# Build OLS, fit model to data ç”¨OLSæ–¹æ³•å»ºæ¨¡è®¡ç®—å‡ºå›å½’çº¿
	OLS = ols(formula = ols_formula, data = ols_data)
	model = OLS.fit()	
	```

- E å„é¡¹ç»Ÿè®¡æŒ‡æ ‡çš„å«ä¹‰

### variable selection

é€‰æ‹©åŒ…å«ä»€ä¹ˆç‰¹å¾/å½±å“å› ç´ /è‡ªå˜é‡åˆ°å›å½’æ¨¡å‹é‡Œã€‚æ ¹æ®å»ºæ¨¡åçš„æŒ‡æ ‡æ•°å€¼ï¼Œè°ƒæ•´æ¨¡å‹ã€‚

- underfittingå’Œoverfitting
	
	R<sup>2</sup>å¤ªä½æˆ–å¤ªä½æˆ–å¤ªé«˜<br>
	å¤ªä½ç­‰äºå›å½’æ¨¡å‹æ²¡æœ‰æŠ“ä½æ ·æœ¬çš„ç‰¹å¾ï¼Œä¹Ÿå°±çº¦ç­‰äºæ‹Ÿåˆæ— æ•ˆ<br>
	å¤ªé«˜åˆ™æœ‰å¯èƒ½æ˜¯å› ä¸ºå¤ªè´´åˆæ ·æœ¬çš„ç‰¹å¾ï¼Œæ‰€ä»¥åè€Œæ— æ³•å»¶å±•å‡ºæ•´ä½“çš„ç‰¹å¾ï¼Œä¸èƒ½å¾ˆå¥½åœ°ç”¨äºé¢„æµ‹æœªçŸ¥æ•°æ®ç»„
	
- Adjusted R-squared value

	R<sup>2</sup>ä¼šéšç€æ ·æœ¬æ•°é‡å¢åŠ è€Œè‡ªç„¶è¶‹è¿‘äº1ï¼ŒAjusted R<sup>2</sup>å»é™¤äº†æ ·æœ¬æ•°é‡å’Œç‰¹å¾ï¼ˆè‡ªå˜é‡ï¼‰æ•°é‡å¯¹è¯„åˆ†çš„å½±å“ï¼Œæ‰€ä»¥æ›´å¥½ç”¨ã€‚

- å¸¸è§ç­›é€‰æ–¹æ³•
	- forward selection & backward elimiation
	
		forwardæ˜¯ä»ç¬¬ä¸€ä¸ªå¯èƒ½çš„ç‰¹å¾/å› ç´ /è‡ªå˜é‡å¼€å§‹ï¼Œä¸€ä¸ªä¸€ä¸ªåˆ¤æ–­æ˜¯å¦è¦åŒ…å«<br>
		backwardæ˜¯å…ˆåŒ…å«æ‰€æœ‰å¯èƒ½çš„ç‰¹å¾ï¼Œå†ä»æœ€åä¸€ä¸ªå¼€å§‹åˆ¤æ–­æ˜¯ä¸æ˜¯è¦å‰”é™¤

	- based on Extra-sum-of-squares F-test
		
		based on p-value

- Regularization æ­£åˆ™åŒ–
	
	è§£å†³è¿‡åº¦æ‹Ÿåˆçš„æ¨¡å‹ï¼Œé™ä½varianceå¢åŠ bias<br>
	Lasso Regression å»æ‰æ‰€æœ‰å¯¹é¢„æµ‹Yä¸å¤ªæœ‰ç”¨çš„ç‰¹å¾<br>
	Ridge Regression é™ä½ä¸é‡è¦ç‰¹å¾çš„å½±å“ä½†ä¸ä¼šå»æ‰å®ƒä»¬
	
	Elastic Net regression æµ‹è¯•lassoå’Œridgeå“ªä¸ªæˆ–æ··åˆæ¨¡å¼æ›´å¥½
	
	Principal component analysis (PCA) é˜…è¯»ææ–™é‡Œçš„æ¦‚å¿µ

## M4 Advanced hypothesis testing å‡è®¾æ£€éªŒ
### chi-squared test Ï‡<sup>2</sup>å¡æ–¹æ£€éªŒ

ç”¨äºæ£€éªŒä¸åˆ†ç±»å˜é‡ç›¸å…³çš„å‡è®¾

- Ï‡<sup>2</sup> Goodness of fit test å¡æ–¹é€‚åˆåº¦æ£€éªŒ

	è§‚æµ‹å€¼æ˜¯å¦ç¬¦åˆé¢„æœŸåˆ†å¸ƒè§„å¾‹<br>
	Null hypothesis(H<sub>0</sub>) è§‚æµ‹å€¼ç¬¦åˆé¢„æœŸåˆ†å¸ƒè§„å¾‹<br>
	Alternative hypothesis(H<sub>1</sub>) ä¸ç¬¦åˆé¢„æœŸåˆ†å¸ƒ
	
	å¡æ–¹å€¼è®¡ç®—å…¬å¼<br>
	Ï‡<sup>2</sup> = Î£((observed-expected)<sup>2</sup>/expedted)
	
	è‡ªç”±åº¦ï¼Œåˆ†ç±»æ•°-1
	
	å†ç»§ç»­è‹¥å¹²æ­¥è®¡ç®—åæŸ¥å‡ºP-valueï¼Œæ ¹æ®ç½®ä¿¡åº¦å†³å®šæ‹’ç»æˆ–æ¥å—H<sub>0</sub><br>
	P<ç½®ä¿¡åº¦ï¼Œæ‹’ç»H<sub>0</sub>
	
	expected valuesåˆ†ç±»æ•°å°äº5æ—¶ï¼Œé€‚åˆåº¦æ£€éªŒå¯èƒ½ä¸å‡†ç¡®ã€‚
	
	```python
	import scipy.stats as stats
	observations = [650, 570, 420, 480, 510, 380, 490]
	expectations = [500, 500, 500, 500, 500, 500, 500]
	result = stats.chisquare(f_obs=observations, f_exp=expectations)
	```
	
- Ï‡<sup>2</sup> Test for independence å¡æ–¹ç‹¬ç«‹æ€§æ£€éªŒ

	æ£€éªŒä¸¤ä¸ªåˆ†ç±»å˜é‡ä¹‹é—´æ˜¯å¦ç›¸äº’ç‹¬ç«‹<br>
	Null hypothesis(H<sub>0</sub>) ä¸¤ä¸ªåˆ†ç±»å˜é‡ä¹‹é—´ç›¸äº’ç‹¬ç«‹<br>
	Alternative hypothesis(H<sub>1</sub>) ä¸¤ä¸ªåˆ†ç±»å˜é‡äº’ç›¸å½±å“
	
	çŸ©é˜µï¼Œæ¨ªåˆ—æ˜¯å˜é‡1çš„è‹¥å¹²ç§æƒ…å†µï¼Œçºµåˆ—æ˜¯å˜é‡2çš„è‹¥å¹²ç§æƒ…å†µ<br>
	æ¨ªçºµäº¤å‰ç‚¹ä¸Šè®°å½•è§‚æµ‹æ•°é‡çš„ç´¯è®¡å€¼<br>
	å†è®¡ç®—æ¯ä¸ªäº¤å‰ç‚¹çš„æœŸæœ›å€¼E<sub>ij</sub> = R<sub>i</sub>_sum*C<sub>j</sub>_sum/Total
	
	å¡æ–¹å€¼è®¡ç®—åŒä¸Š<br>
	è‡ªç”±åº¦ï¼Œæ¨ªåˆ†ç±»æ•°mï¼Œçºµåˆ†ç±»æ•°nï¼Œ(m-1)*(n-1)<br>
	P-valueåˆ¤æ–­æ‹’ç»æˆ–æ¥å—å‡è®¾åŒä¸Š
	
	```python
	import numpy as np
	import scipy.stats as stats
	observations = np.array([[850, 450],[1300, 900]])
	result = stats.contingency.chi2_contingency(observations, correction=False)
	# result output order: the ğ›¸2 statistic, p-value, degrees of freedom, and expected values in array format
	```
	
	